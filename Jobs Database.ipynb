{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jobs Database - Computer Vision Domain\n",
    "# NEU Skunkwork AI Project\n",
    "\n",
    "Yunan Shao  \n",
    "1818832\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Looking for jobs or internships seems a task of its own and the search is no longer based on sole\n",
    "fulfillment of the required job skills, but a lot of networking and recommendations is involved\n",
    "around too. The amount of work involved in finding the correct job builds a great amount of anxiety\n",
    "among the job seekers and the recruiters who want the right talent for their company.  \n",
    "There are two concerns that are to be addressed here. First, matching the job seekers with the right\n",
    "employers and second, provide guidance to aspiring job seekers on the skills that are in demand so\n",
    "that they can build them to stay relevant in the job market.  \n",
    "The job providers and job seekers form a large amount of data which provides for many interesting\n",
    "trends for analysis and interpretation to make the most of data available.  \n",
    "With the data currently available from the seekers and providers, these pitfalls can be fixed. The\n",
    "presence of information on job skills, salaries and user tendencies in many existing websites such\n",
    "as Indeed, LinkedIn, Glassdoor etc can be utilized to match people to positions which may seem\n",
    "simply impossible without using AI to analyze data.  \n",
    "The jobs database would be a one stop solution to reduce the job search and talent acquisition\n",
    "stress levels. Artificial intelligence (AI) and machine learning can be utilized for complex task of\n",
    "matching work to talent so that it is efficient and less resume spamming.  \n",
    "\n",
    "## Goals (Original)\n",
    "1. Scraping company links in target domain\n",
    "2. Scraping jobs using the output from company list\n",
    "3. Extract skill set for jobs and store job information into database\n",
    "4. Generate a word cloud for the desired skills in the domain\n",
    "\n",
    "## Conclusions\n",
    "* I will break my conclusion into different sections based on my pattern. The machine learning part will be the first one since this section is the most relevant to my current course.  \n",
    "* The overall result achieve the goals according to my understanding of this project. Some of the approaches can be improved such as better design of scrapers, data normalization, skill extraction using machine learning model. However, I don't have enough time to implement them as an individual team.\n",
    "* Instructions of running the codes and some of the detailed explainations are in realted notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "### Machine Learning\n",
    "#### Classification Model\n",
    "I created a simple classification model which can be used to predict the job title based on a set of skills. I choose to use SVM as my model because the target variable is not linearly separable. The implementations are:\n",
    "1. Vectorize the skill set and label encode the job title as numeric values\n",
    "2. Train the data\n",
    "3. Cross-validate the model\n",
    "4. Test the model and get the prediction with vectorized skill set, inverse transform the result into human readable text.  \n",
    "The score of cross validation is not very ideal. Because the job titles are not normalized enough, but it needs more time in order to inspect the dataset.\n",
    "\n",
    "#### Clustering with Bag of Words Model\n",
    "I think this one is an important part because I can extract the key words by using machine learning model without spending a lot of time inspecting the data. I only finished half of my design since there is no enough time. But I can explain the approcah here:\n",
    "1. Set a list of keywords such as 'education', 'skill' as trigger words and then use N-grams to gather the words. Current range [2,4]\n",
    "2. Perform a coarse clustering using KNN on stemmed N-grams\n",
    "3. Clusters generated can be labled into different categories (example in notebook)\n",
    "4. Reclustering on each cluster generated from previous step\n",
    "5. Use skill terms to match\n",
    "\n",
    "#### Recommandation Model\n",
    "As most of teams did, I set creating a recommandation model as my machine learning study in my initial task list. However, I created the classification model instead because:\n",
    "1. An ideal recommandation model (in my opinion) should consider not only on the jobs data but also user data such as current job, year of experience etc. Creating the recommandation model only use the skill sets doesn't quite make sense to me. If I want to search for the job based on my skills, I can simple perform a query on the database I created.\n",
    "2. The project focuses more on the key words in a specific domain, after filtering and analysing, the number of jobs I collected is around 700 which I don't think it's necessary to create the recommandation model for it.\n",
    "\n",
    "### Scraping\n",
    "\n",
    "In the beginning, I built a scraper for getting a company list from angel.co. There are other websites which contains information of companies such as Linkedin and Crunchbase. However, the structure of Linkedin doesn't support searching companies by domain very well (can't find useful result for computer vision at least). Crunchbase is a good resource and the web structure is easier for scraping. But it requires monthly payment for data access. Then, I use another scraper for searching the listed jobs for the companies from glassdoor.com. I think one website is already enough because using multiple websites could result duplicate records.\n",
    "\n",
    "#### Problem\n",
    "1. Social media pages don't contain information about jobs.\n",
    "2. The jobs listed by company may not be relevant to the domain. The final result I want to get from the raw data is a set of jobs with the desired 'professional' skills. The jobs listed by company could be sales or other titles that are not relevant to the domain.\n",
    "\n",
    "#### Improvement & Result\n",
    "I used one scraper only for keyword searching for multiple times. The advantage is I can get related jobs and it is not necessary that the companies list them are related in the domain. For example, Apple doesn't focus only in computer vision domain and if I searched the company by domain, it may not appear in my company list. But it's AI, camera team still have listed jobs which are related to this domain.  \n",
    "The combined raw dataset using key words 'Computer Vision Enginner', 'Computer Vision Scientist' and 'Computer Vision' has about 2000 jobs listed on glassdoor.com.\n",
    "\n",
    "### Data Clean & Database Design\n",
    "Raw Fields: Job Title, Location, Company Name, Job Description, Salary\n",
    "Database Fields: Job Title, Location, Company Name, Job Description, Salary, Skills, Education Level\n",
    "\n",
    "#### Problem\n",
    "The original raw data set that I got has about 2000 jobs by combining the result of 'Computer Vision', 'Computer Vision Engineer' and 'Computer Vision Scientist'. But there are still many jobs are not related to the domain such as frontend programmer.\n",
    "\n",
    "#### Improvement & Result\n",
    "After combining the raw data, I filter the job titles using a set of computer vision related keywords. The final dataset has about 800 jobs.  \n",
    "I extract the basic programming skills and education level from the raw data and store the combined data into a MySQL database.\n",
    "\n",
    "### Word Cloud\n",
    "\n",
    "#### Problem\n",
    "Because glassdoor doesn't have a fixed web element contains the desired skills, I scrapted the complete job description instead. The job description contains many texts are not related to job skills which means that the common steps of generating a word cloud do not work very well. The common steps include: tokenizing, removing stop words, generating word cloud based on the content with clustering.\n",
    "\n",
    "#### Improvement\n",
    "During progress report, professor Brown suggested that I could get some related words from Wikipedia page. I went through the page and get a common knowledge of what technologies are related to computer vision. In addtion, I use mapreduce to get the word count, and build dictionaries for extracting keywords.   \n",
    "Customized Vocabulary Dictionaries:\n",
    "1. Programming Languages\n",
    "2. Education\n",
    "3. Analysing Softwares/Tools\n",
    "4. Big-Data Technologies\n",
    "5. Machine Learning Models\n",
    "6. Other Keywords Based on The Occurance from MR Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Workflow\n",
    "### Current Workflow Diagram\n",
    "\n",
    "<img src='./00-data/workflow.jpg'>\n",
    "\n",
    "### Word Cloud\n",
    "<img src='./00-data/after.png'>\n",
    "\n",
    "### SVM Model\n",
    "<img src='./00-data/SVM.jpg'>\n",
    "\n",
    "### Bag of Words Design\n",
    "<img src='./00-data/bow.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "Codes marked as Deprecated are just for demonstrating a different approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 - Scapers\n",
    "1. [Job Scraper with Keywords](./01-scraper/job_search_direct.ipynb)\n",
    "2. [Company Scraper (Deprecated)](./01-scraper/company_search.ipynb)\n",
    "3. [Job Scraper with Company Scraper Outputs (Deprecated)](./01-scraper/job_search_company.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 - Data Clean & Database\n",
    "1. [Data Clean](./02-database/db_clean.ipynb)\n",
    "2. [Database Import](./02-database/db_import.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 - Word Cloud\n",
    "\n",
    "1. [Word Cloud by Customized Vocabulary](./03-wordcloud/wordcloud_vocabulary_dict.ipynb)\n",
    "2. [Word Cloud by MapReduce (Deprecated)](./03-wordcloud/wordcloud_mr_wordcount.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 - Classification\n",
    "\n",
    "1. [SVM](./04-classification/classification_prediction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 - Bag of Words (Incomplete)\n",
    "\n",
    "1. [Description Extraction](./05-bagofwords/01-job_description_extraction.ipynb)\n",
    "2. [Coarse Clustering](./05-bagofwords/02-feature_clustering.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions \n",
    "-By own: 80%  \n",
    "-By online resources: 20%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "1. https://github.com/natmod/glassdoor-scrape\n",
    "2. https://github.com/rodrigosnader/angel-scraper\n",
    "3. https://github.com/datamusing/employment_skills_extraction\n",
    "4. https://github.com/2dubs/Job-Skills-Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "Apply to all notebooks \n",
    "\n",
    "Copyright 2019 COPYRIGHT Yunan Shao\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
